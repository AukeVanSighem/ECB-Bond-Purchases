{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of modules that are not installed in the course\n",
    "!pip install OpenPermID\n",
    "!pip install Levenshtein\n",
    "!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:54:26.868352Z",
     "start_time": "2021-04-24T11:54:26.855355Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "source": [
    "# Downloading all bonds ever owned in CSPP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the csv from the url and places the new data in a dictionary with keys = ISIN,\n",
    "# and value = [NCB, ISSUER, MATURITY DATE, COUPON RATE]\n",
    "def downloadDataToDictionary(url,dictionary):\n",
    "    r = requests.get(url) # create HTTP response object\n",
    "    nameCompany = '' # make a string for the company name \n",
    "                     # (do this here so that is in scope of whole function)\n",
    "    if r.status_code != 200: return # if website wasn't accessed in the right way, \n",
    "                                    # stop the function\n",
    "    # this for loop loops through all the lines of the retrieved csv-file, except for the heading\n",
    "    for line in r.text.split('\\r\\n')[1:]:\n",
    "        if not re.search(r'[a-z]',line): continue # if the line doesn't contain letters, \n",
    "                                                  # go to the next line\n",
    "        if re.search(r',+$',line): line = re.sub(r',+$',r'',line) # remove commas at end of line\n",
    "        splitLine = line.split(',')\n",
    "        if len(splitLine) < 5: continue # We expect at least 5 items as we want 5 columns \n",
    "                                        # and name could lead to additional columns\n",
    "        if re.search(r'(?:\\\".*,.*\\\")',line): # searches commas between \" as these are part of the name \n",
    "                                             # and shouldn't be split\n",
    "            nameCompany = re.search(r'(?:\\\".*,.*\\\")',line).group(0) \n",
    "                                             # name of the company is between the \"\"\n",
    "            nameCompany = re.sub(r\"\\\"\",\"\",nameCompany) # remove the \"\"\n",
    "        else:\n",
    "            for str in splitLine:\n",
    "                re.sub('\\\"','',str)\n",
    "            nameCompany = splitLine[2]\n",
    "        if (splitLine[1] not in dictionary): # only add new ISINs to the dictionary\n",
    "            dictionary[splitLine[1]] = [splitLine[0], nameCompany, splitLine[-2], splitLine[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateToDownload = datetime.date(2017, 6, 23)\n",
    "change_url_date = datetime.date(2020, 3, 27)\n",
    "end_date = datetime.date.today()\n",
    "delta = datetime.timedelta(days=7)\n",
    "dictionaryBondsECB = {}\n",
    "\n",
    "while dateToDownload <= change_url_date:\n",
    "    date = dateToDownload.strftime(\"%Y%m%d\")\n",
    "    url = \"https://www.ecb.europa.eu/mopo/pdf/CSPPholdings_\"+date+\".csv\"\n",
    "    downloadDataToDictionary(url,dictionaryBondsECB)\n",
    "    dateToDownload += delta\n",
    "dateToDownload+delta\n",
    "while dateToDownload <= end_date:\n",
    "    date = dateToDownload.strftime(\"%Y%m%d\")\n",
    "    url = \"https://www.ecb.europa.eu/mopo/pdf/CSPP_PEPP_corporate_bond_holdings_\"+date+\".csv\"\n",
    "    downloadDataToDictionary(url,dictionaryBondsECB)\n",
    "    dateToDownload += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixData = [] # 2D array with row per ISIN and columns for different data\n",
    "for ISIN, dataInDictionary in dictionaryBondsECB.items():\n",
    "    item = [ISIN] + dataInDictionary\n",
    "    matrixData.append(item)\n",
    "holdingsECB = pd.DataFrame(matrixData, columns=[\"ISIN\",\"NCB\",\"ISSUER\",\"MATURITY DATE\",\"COUPON RATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECB green bonds that are listed on Euronext stock exchange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data of green bonds listed on Euronext \n",
    "euronext_greenbonds = pd.read_excel(\"data/Euronext-Green-Bond-List.xlsx\", header=0)\n",
    "euronext_greenbonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the ISIN of the Euronext green bonds \n",
    "euronext_greenbond_isin = euronext_greenbonds[\"ISIN\"]\n",
    "euronext_greenbond_isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the ISINs of the ECB and Euronext green bonds and returning matches \n",
    "ecbgreenbonds = holdingsECB[(holdingsECB[\"ISIN\"].isin(euronext_greenbond_isin))]\n",
    "ecbgreenbonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which bonds are green?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T13:46:56.431566Z",
     "start_time": "2021-04-11T13:46:56.404371Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading data of which bonds are held by ECB at 2nd of april 2021\n",
    "holdings20210402 = pd.read_csv(\"data/CSPP_PEPP_corporate_bond_holdings_20210402.csv\", header=0, encoding='latin-1')\n",
    "holdings20210402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T13:46:56.461915Z",
     "start_time": "2021-04-11T13:46:56.432565Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading data of all green bonds in the world (ICMA - 06 april 2021)\n",
    "greenBonds = pd.read_csv(\"data/ICMA-Sustainable-Bonds-Database-060421.csv\", delimiter=\";\", encoding=\"latin-1\")\n",
    "greenBonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T13:47:14.732545Z",
     "start_time": "2021-04-11T13:46:56.462912Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate lists of all companies that have green bonds owned by ECB and all companies that have non-green bonds owned by ECB\n",
    "companiesECBSet = set(holdings20210402[\"ISSUER\"].tolist()) # set of companies with bonds owned by ECB\n",
    "greenBondsCompaniesSet = set(greenBonds[\"Green Bond issuer\"].tolist()) # set of companies with green bonds\n",
    "greenCompanies = [] # this list will hold all companies with green bonds owned by ECB\n",
    "for company in companiesECBSet:\n",
    "    for greenBondCompany in greenBondsCompaniesSet:\n",
    "        # Here, regex are used because names don't exactly match\n",
    "        if (re.search(\".*\"+greenBondCompany+\".*\", company) or re.search(\".*\"+company+\".*\",greenBondCompany)):\n",
    "            greenCompanies.append(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenFIGI to convert bond ISIN to ticker\n",
    "FIGIs are unique identifiers of financial instruments issued by Bloomberg. OpenFIGI is an API that maps third-party identifiers to FIGI, but it also returns other information such as company name and ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:36:09.642571Z",
     "start_time": "2021-04-14T14:36:09.611570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:40:32.821846Z",
     "start_time": "2021-04-14T14:40:32.807845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Functions\n",
    "def map_jobs(jobs):\n",
    "    handler = urllib.request.HTTPHandler()\n",
    "    opener = urllib.request.build_opener(handler)\n",
    "    openfigi_url = 'https://api.openfigi.com/v3/mapping'\n",
    "    request = urllib.request.Request(openfigi_url, data=bytes(json.dumps(jobs), encoding='utf-8'))\n",
    "    request.add_header('Content-Type','application/json')\n",
    "    if openfigi_apikey:\n",
    "        request.add_header('X-OPENFIGI-APIKEY', openfigi_apikey)\n",
    "    request.get_method = lambda: 'POST'\n",
    "    connection = opener.open(request)\n",
    "    if connection.code != 200:\n",
    "        raise Exception('Bad response code {}'.format(str(response.status_code)))\n",
    "    return json.loads(connection.read().decode('utf-8'))\n",
    "\n",
    "def job_results_handler(jobs, job_results):\n",
    "    df = pd.DataFrame({})\n",
    "    for job, result in zip(jobs, job_results):\n",
    "        job_df = pd.DataFrame({'ISIN': [job['idValue']]})\n",
    "        results_df = pd.read_json(json.dumps(result.get('data', [])))\n",
    "        df = pd.concat([df, pd.concat([job_df, results_df], axis = 1)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:42:25.757840Z",
     "start_time": "2021-04-14T14:42:01.807644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert raw data to appropriate input format\n",
    "ISIN_FIGI = pd.concat([pd.Series(np.tile('ID_ISIN', 1642)), holdings20210402.ISIN], axis = 1)\n",
    "ISIN_FIGI = ISIN_FIGI.rename(columns = {0: 'idType', 'ISIN': 'idValue'})\n",
    "\n",
    "# Map to FIGI\n",
    "openfigi_apikey = 'c89ac66d-e0d2-416f-9c5e-0ed7ec59c770' # This is my personal key (Fred)\n",
    "jobs_per_access = 100\n",
    "no_of_access = len(ISIN_FIGI)//jobs_per_access + 1\n",
    "\n",
    "figi = pd.DataFrame({})\n",
    "for i in range(no_of_access):\n",
    "    lower_bound = jobs_per_access * i\n",
    "    upper_bound = jobs_per_access * (i + 1) if i < no_of_access - 1 else max(ISIN_FIGI.index) + 1\n",
    "    job = ISIN_FIGI.iloc[lower_bound:upper_bound].to_dict(orient = 'records')\n",
    "    job_results = map_jobs(job)\n",
    "    figi = figi.append(job_results_handler(job, job_results))\n",
    "    \n",
    "# Extracting the pure ticker\n",
    "figi.ticker = figi.ticker.apply(lambda x: x.split()[0])\n",
    "figi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of companies supported by ECB with green ESG scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next 4 blocks calculate the score of a given ticker with the function 'web_scraper(ticker)' as shown by the example for Microsoft Corporation (MSFT).\n",
    "TODO: Find the ticker belonging to each company and iterate over them to see if the ESG score or the Environment Score indicate that the given company is green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T13:48:59.100480Z",
     "start_time": "2021-04-11T13:48:59.093563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Source used: https://curt-beck1254.medium.com/scrapping-financial-esg-data-with-python-99d171a12c51\n",
    "from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:50:53.658196Z",
     "start_time": "2021-04-14T14:50:53.638685Z"
    }
   },
   "outputs": [],
   "source": [
    "def web_scraper(ticker):\n",
    "    elements = []\n",
    "    web_data = requests.get('https://finance.yahoo.com/quote/'+ticker+'/sustainability?p='+ticker).text\n",
    "    soup = BeautifulSoup(web_data, 'html.parser')\n",
    "    esg_score = soup.find('div', {'class':'Fz(36px) Fw(600) D(ib) Mend(5px)'})\n",
    "    datapoint = esg_score.text if esg_score != None else np.NaN\n",
    "    controversy_score = soup.find('div', {'class': 'D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)'})\n",
    "    controversy_datapoint = controversy_score.text if controversy_score != None else np.NaN\n",
    "    scores = soup.find_all('div', {'class': 'D(ib) Fz(23px) smartphone_Fz(22px) Fw(600)'})\n",
    "    if len(scores) == 0:\n",
    "        elements = [np.NaN, np.NaN, np.NaN]\n",
    "    else:\n",
    "        for score in scores:\n",
    "            elements.append(score.text)\n",
    "        \n",
    "    df = pd.DataFrame({'Total ESG Score': datapoint,\n",
    "                      'Environment Score': elements[0],\n",
    "                       'Social Score': elements[1],\n",
    "                      'Governance Score': elements[2],\n",
    "                      'Controversy Score': controversy_datapoint},\n",
    "                     index=[ticker])\n",
    "    df = df.astype('float')\n",
    "    df['Controversy Assessment'] = df.apply(lambda x: level(x['Controversy Score']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T13:49:48.507787Z",
     "start_time": "2021-04-11T13:49:48.504101Z"
    }
   },
   "outputs": [],
   "source": [
    "def level(x):\n",
    "    if x == 0.0:\n",
    "        return 'No Controversy'\n",
    "    if x == 1.0:\n",
    "        return 'Little Controverssy'\n",
    "    if x == 2.0:\n",
    "        return 'Moderate Controversy'\n",
    "    if x == 3.0:\n",
    "        return 'Relatively High Controversy'\n",
    "    else:\n",
    "        return 'Severe Controversy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:49:59.425493Z",
     "start_time": "2021-04-14T14:49:58.898943Z"
    }
   },
   "outputs": [],
   "source": [
    "web_scraper('MSFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T14:57:58.062419Z",
     "start_time": "2021-04-14T14:57:50.353078Z"
    }
   },
   "outputs": [],
   "source": [
    "ESG_Summary = pd.DataFrame({})\n",
    "for i in set(figi.ticker.iloc[0:50].tolist()):\n",
    "    ESG_Summary.append(web_scraper(i))\n",
    "ESG_Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Sectors and Locations of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:53:34.210122Z",
     "start_time": "2021-04-24T11:53:33.456064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "from OpenPermID import OpenPermID\n",
    "import Levenshtein\n",
    "import geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:53:58.533147Z",
     "start_time": "2021-04-24T11:53:58.526149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gain access to the permid database\n",
    "opid = OpenPermID()\n",
    "opid.set_access_token(\"r95vEAhvmucG8iNGtsP17hjbgUGMhz4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:58:09.991762Z",
     "start_time": "2021-04-24T11:54:46.321129Z"
    }
   },
   "outputs": [],
   "source": [
    "companies = holdings20210402.ISSUER.astype('string').unique()\n",
    "permid_mappings = pd.DataFrame({})\n",
    "unmapped_companies = []\n",
    "\n",
    "for company in companies:\n",
    "    \n",
    "    # In case of connection error, allow it to try at most 5 times\n",
    "    err, count = 0, 0\n",
    "    while (err != None and count < 5):\n",
    "        output, err = opid.search(company)\n",
    "    if err != None:\n",
    "        unmapped_companies.append(company)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    df = output['organizations']\n",
    "    if len(df) == 0:\n",
    "        permid = np.NaN\n",
    "        name = np.NaN\n",
    "    elif len(df) == 1:\n",
    "        permid = df.iloc[0,0].split('/')[-1]\n",
    "        name = df.iloc[0,1]\n",
    "    else:\n",
    "        # If multiple records are return, choose the record which the company name is the most similar to the keyword\n",
    "        similarityScores = df.organizationName.apply(lambda x: Levenshtein.ratio(company, x))\n",
    "        max_index = similarityScores.idxmax()\n",
    "        permid = df.iloc[max_index, 0].split('/')[-1]\n",
    "        name = df.iloc[max_index, 1]\n",
    "    permid_mappings = permid_mappings.append(pd.DataFrame({'keyword':[company], 'companyName': [name], 'PermID':[permid]}))\n",
    "    \n",
    "permid_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:58:41.894168Z",
     "start_time": "2021-04-24T11:58:21.616340Z"
    }
   },
   "outputs": [],
   "source": [
    "permids = permid_mappings.PermID.dropna().astype('string')\n",
    "sector_lookups = pd.DataFrame({})\n",
    "unsuccessful_lookups = []\n",
    "\n",
    "for permid in permids:\n",
    "    \n",
    "    # In case of connection error, allow it to try at most 5 times\n",
    "    err, count = 0, 0\n",
    "    while (err != None and count < 5):\n",
    "        output, err = opid.lookup(permid)\n",
    "    if err != None:\n",
    "        unsuccessful_lookups.append(permid)\n",
    "        continue\n",
    "\n",
    "    if \"hasPrimaryBusinessSector\" in output.columns:\n",
    "        sector_info = output.loc[:, 'hasPrimaryBusinessSector': 'hasPrimaryIndustryGroup']\n",
    "        sector_info = sector_info.applymap(lambda x: x.split('/')[-1])\n",
    "    if \"isIncorporatedIn\" in output.columns:\n",
    "        loc_info = output.loc[:, 'isIncorporatedIn': 'isDomiciledIn']\n",
    "        loc_info = loc_info.applymap(lambda x: x.split('/')[-2])\n",
    "        \n",
    "    row = pd.DataFrame({'PermID': [permid]})\n",
    "    row = pd.concat([row, sector_info], axis = 1) if type(sector_info) == pd.DataFrame else row\n",
    "    row = pd.concat([row, loc_info], axis = 1) if type(loc_info) == pd.DataFrame else row\n",
    "    sector_lookups = sector_lookups.append(row)\n",
    "    \n",
    "    sector_info, loc_info = None, None\n",
    "\n",
    "sector_lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:59:20.465674Z",
     "start_time": "2021-04-24T11:59:14.137948Z"
    }
   },
   "outputs": [],
   "source": [
    "sector_lookups_converted = sector_lookups.copy()\n",
    "sector_types = sector_lookups.columns[1:4]\n",
    "for sector_type in sector_types:\n",
    "    sector_dict = {}\n",
    "    sectors = sector_lookups.loc[:, sector_type].dropna().astype('string').unique()\n",
    "    for sector in sectors:\n",
    "        output, err = opid.lookup(sector)\n",
    "        sector_dict[sector] = output.iloc[0, -1]\n",
    "    sector_lookups_converted[sector_type] = sector_lookups[sector_type].fillna('missing').astype('string').apply(lambda x: np.NaN if x == 'missing' else sector_dict[x])\n",
    "\n",
    "loc_types = sector_lookups.columns[4:]\n",
    "for loc_type in loc_types:\n",
    "    loc_dict = {}\n",
    "    locs = sector_lookups.loc[:, loc_type].dropna().astype('string').unique()\n",
    "    for loc in locs:\n",
    "        g = geocoder.geonames(loc, method='details', key='brian1998716')\n",
    "        loc_dict[loc] = g.address\n",
    "    sector_lookups_converted[loc_type] = sector_lookups[loc_type].fillna('missing').astype('string').apply(lambda x: np.NaN if x == 'missing' else loc_dict[x])\n",
    "\n",
    "sector_lookups_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T11:59:30.840538Z",
     "start_time": "2021-04-24T11:59:30.805529Z"
    }
   },
   "outputs": [],
   "source": [
    "sector_mappings = pd.merge(permid_mappings, sector_lookups_converted, how = 'left', on = 'PermID')\n",
    "sector_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T12:01:22.607978Z",
     "start_time": "2021-04-24T12:01:22.577963Z"
    }
   },
   "outputs": [],
   "source": [
    "sector_mappings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all industries in which ECB invested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import all the data into one big data frame (done)\n",
    "2) Search for all the ISINs\n",
    "3) Find \"green bond\", \"ESG bond\" and most importantly \"sector\"\n",
    "4) Analyse sectoer ?! --> TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:14:28.478392Z",
     "start_time": "2021-04-25T09:14:28.474404Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T09:14:31.546297Z",
     "start_time": "2021-04-25T09:14:30.120110Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_files = os.listdir(\"data/Sector_data/\")\n",
    "sector_data = pd.DataFrame({})\n",
    "for file in folder_files:\n",
    "    if (file != '.ipynb_checkpoints'): # TODO: find cleaner way\n",
    "        sector_data_extra = pd.ExcelFile(\"data/Sector_data/\"+file)\n",
    "        sector_data = sector_data.append(sector_data_extra.parse())\n",
    "print(sector_data.count)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "name": "python389jvsc74a57bd0c66e68dc8effbb73dddbef0493505d10f36de5f905f8b8ed3ac14ee9c27e255b",
   "display_name": "Python 3.8.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "c66e68dc8effbb73dddbef0493505d10f36de5f905f8b8ed3ac14ee9c27e255b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}